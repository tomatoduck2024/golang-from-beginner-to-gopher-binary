"use strict";(self.webpackChunkgolang_from_beginner_to_gopher=self.webpackChunkgolang_from_beginner_to_gopher||[]).push([[3625],{2739:(t,n,e)=>{e.r(n),e.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"go_scheduler/gmp","title":"GMP Model","description":"Explanation of Go\'s GMP (Goroutine, Machine, Processor) model for concurrency.","source":"@site/docs/concurrency/go_scheduler/gmp.md","sourceDirName":"go_scheduler","slug":"/go_scheduler/gmp","permalink":"/golang-from-beginner-to-gopher-binary/docs/concurrency/go_scheduler/gmp","draft":false,"unlisted":false,"editUrl":"https://github.com/tomatoduck2024/golang-from-beginner-to-gopher/tree/main/docs/concurrency/go_scheduler/gmp.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"concurrencySidebar","previous":{"title":"Ticker","permalink":"/golang-from-beginner-to-gopher-binary/docs/concurrency/ticker"}}');var o=e(4848),a=e(8453);const s={sidebar_position:2},r="GMP Model",c={},l=[];function u(t){const n={code:"code",h1:"h1",header:"header",p:"p",pre:"pre",...(0,a.R)(),...t.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"gmp-model",children:"GMP Model"})}),"\n",(0,o.jsx)(n.p,{children:"Explanation of Go's GMP (Goroutine, Machine, Processor) model for concurrency."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-go",children:"type gobuf struct {\n\t// The offsets of sp, pc, and g are known to (hard-coded in) libmach.\n\t//\n\t// ctxt is unusual with respect to GC: it may be a\n\t// heap-allocated funcval, so GC needs to track it, but it\n\t// needs to be set and cleared from assembly, where it's\n\t// difficult to have write barriers. However, ctxt is really a\n\t// saved, live register, and we only ever exchange it between\n\t// the real register and the gobuf. Hence, we treat it as a\n\t// root during stack scanning, which means assembly that saves\n\t// and restores it doesn't need write barriers. It's still\n\t// typed as a pointer so that any other writes from Go get\n\t// write barriers.\n\tsp   uintptr\n\tpc   uintptr\n\tg    guintptr\n\tctxt unsafe.Pointer\n\tret  uintptr\n\tlr   uintptr\n\tbp   uintptr // for framepointer-enabled architectures\n}\n\n\ntype gobuf struct {\n\t// The offsets of sp, pc, and g are known to (hard-coded in) libmach.\n\t//\n\t// ctxt is unusual with respect to GC: it may be a\n\t// heap-allocated funcval, so GC needs to track it, but it\n\t// needs to be set and cleared from assembly, where it's\n\t// difficult to have write barriers. However, ctxt is really a\n\t// saved, live register, and we only ever exchange it between\n\t// the real register and the gobuf. Hence, we treat it as a\n\t// root during stack scanning, which means assembly that saves\n\t// and restores it doesn't need write barriers. It's still\n\t// typed as a pointer so that any other writes from Go get\n\t// write barriers.\n\tsp   uintptr\n\tpc   uintptr\n\tg    guintptr\n\tctxt unsafe.Pointer\n\tret  uintptr\n\tlr   uintptr\n\tbp   uintptr // for framepointer-enabled architectures\n}\n\n// sudog (pseudo-g) represents a g in a wait list, such as for sending/receiving\n// on a channel.\n//\n// sudog is necessary because the g \u2194 synchronization object relation\n// is many-to-many. A g can be on many wait lists, so there may be\n// many sudogs for one g; and many gs may be waiting on the same\n// synchronization object, so there may be many sudogs for one object.\n//\n// sudogs are allocated from a special pool. Use acquireSudog and\n// releaseSudog to allocate and free them.\ntype sudog struct {\n\t// The following fields are protected by the hchan.lock of the\n\t// channel this sudog is blocking on. shrinkstack depends on\n\t// this for sudogs involved in channel ops.\n\n\tg *g\n\n\tnext *sudog\n\tprev *sudog\n\telem unsafe.Pointer // data element (may point to stack)\n\n\t// The following fields are never accessed concurrently.\n\t// For channels, waitlink is only accessed by g.\n\t// For semaphores, all fields (including the ones above)\n\t// are only accessed when holding a semaRoot lock.\n\n\tacquiretime int64\n\treleasetime int64\n\tticket      uint32\n\n\t// isSelect indicates g is participating in a select, so\n\t// g.selectDone must be CAS'd to win the wake-up race.\n\tisSelect bool\n\n\t// success indicates whether communication over channel c\n\t// succeeded. It is true if the goroutine was awoken because a\n\t// value was delivered over channel c, and false if awoken\n\t// because c was closed.\n\tsuccess bool\n\n\t// waiters is a count of semaRoot waiting list other than head of list,\n\t// clamped to a uint16 to fit in unused space.\n\t// Only meaningful at the head of the list.\n\t// (If we wanted to be overly clever, we could store a high 16 bits\n\t// in the second entry in the list.)\n\twaiters uint16\n\n\tparent   *sudog // semaRoot binary tree\n\twaitlink *sudog // g.waiting list or semaRoot\n\twaittail *sudog // semaRoot\n\tc        *hchan // channel\n}\n\ntype libcall struct {\n\tfn   uintptr\n\tn    uintptr // number of parameters\n\targs uintptr // parameters\n\tr1   uintptr // return values\n\tr2   uintptr\n\terr  uintptr // error number\n}\n\n// Stack describes a Go execution stack.\n// The bounds of the stack are exactly [lo, hi),\n// with no implicit data structures on either side.\ntype stack struct {\n\tlo uintptr\n\thi uintptr\n}\n\n// heldLockInfo gives info on a held lock and the rank of that lock\ntype heldLockInfo struct {\n\tlockAddr uintptr\n\trank     lockRank\n}\n\ntype g struct {\n\t// Stack parameters.\n\t// stack describes the actual stack memory: [stack.lo, stack.hi).\n\t// stackguard0 is the stack pointer compared in the Go stack growth prologue.\n\t// It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.\n\t// stackguard1 is the stack pointer compared in the //go:systemstack stack growth prologue.\n\t// It is stack.lo+StackGuard on g0 and gsignal stacks.\n\t// It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).\n\tstack       stack   // offset known to runtime/cgo\n\tstackguard0 uintptr // offset known to liblink\n\tstackguard1 uintptr // offset known to liblink\n\n\t_panic    *_panic // innermost panic - offset known to liblink\n\t_defer    *_defer // innermost defer\n\tm         *m      // current m; offset known to arm liblink\n\tsched     gobuf\n\tsyscallsp uintptr // if status==Gsyscall, syscallsp = sched.sp to use during gc\n\tsyscallpc uintptr // if status==Gsyscall, syscallpc = sched.pc to use during gc\n\tsyscallbp uintptr // if status==Gsyscall, syscallbp = sched.bp to use in fpTraceback\n\tstktopsp  uintptr // expected sp at top of stack, to check in traceback\n\t// param is a generic pointer parameter field used to pass\n\t// values in particular contexts where other storage for the\n\t// parameter would be difficult to find. It is currently used\n\t// in four ways:\n\t// 1. When a channel operation wakes up a blocked goroutine, it sets param to\n\t//    point to the sudog of the completed blocking operation.\n\t// 2. By gcAssistAlloc1 to signal back to its caller that the goroutine completed\n\t//    the GC cycle. It is unsafe to do so in any other way, because the goroutine's\n\t//    stack may have moved in the meantime.\n\t// 3. By debugCallWrap to pass parameters to a new goroutine because allocating a\n\t//    closure in the runtime is forbidden.\n\t// 4. When a panic is recovered and control returns to the respective frame,\n\t//    param may point to a savedOpenDeferState.\n\tparam        unsafe.Pointer\n\tatomicstatus atomic.Uint32\n\tstackLock    uint32 // sigprof/scang lock; TODO: fold in to atomicstatus\n\tgoid         uint64\n\tschedlink    guintptr\n\twaitsince    int64      // approx time when the g become blocked\n\twaitreason   waitReason // if status==Gwaiting\n\n\tpreempt       bool // preemption signal, duplicates stackguard0 = stackpreempt\n\tpreemptStop   bool // transition to _Gpreempted on preemption; otherwise, just deschedule\n\tpreemptShrink bool // shrink stack at synchronous safe point\n\n\t// asyncSafePoint is set if g is stopped at an asynchronous\n\t// safe point. This means there are frames on the stack\n\t// without precise pointer information.\n\tasyncSafePoint bool\n\n\tpaniconfault bool // panic (instead of crash) on unexpected fault address\n\tgcscandone   bool // g has scanned stack; protected by _Gscan bit in status\n\tthrowsplit   bool // must not split stack\n\t// activeStackChans indicates that there are unlocked channels\n\t// pointing into this goroutine's stack. If true, stack\n\t// copying needs to acquire channel locks to protect these\n\t// areas of the stack.\n\tactiveStackChans bool\n\t// parkingOnChan indicates that the goroutine is about to\n\t// park on a chansend or chanrecv. Used to signal an unsafe point\n\t// for stack shrinking.\n\tparkingOnChan atomic.Bool\n\t// inMarkAssist indicates whether the goroutine is in mark assist.\n\t// Used by the execution tracer.\n\tinMarkAssist bool\n\tcoroexit     bool // argument to coroswitch_m\n\n\traceignore    int8  // ignore race detection events\n\tnocgocallback bool  // whether disable callback from C\n\ttracking      bool  // whether we're tracking this G for sched latency statistics\n\ttrackingSeq   uint8 // used to decide whether to track this G\n\ttrackingStamp int64 // timestamp of when the G last started being tracked\n\trunnableTime  int64 // the amount of time spent runnable, cleared when running, only used when tracking\n\tlockedm       muintptr\n\tsig           uint32\n\twritebuf      []byte\n\tsigcode0      uintptr\n\tsigcode1      uintptr\n\tsigpc         uintptr\n\tparentGoid    uint64          // goid of goroutine that created this goroutine\n\tgopc          uintptr         // pc of go statement that created this goroutine\n\tancestors     *[]ancestorInfo // ancestor information goroutine(s) that created this goroutine (only used if debug.tracebackancestors)\n\tstartpc       uintptr         // pc of goroutine function\n\tracectx       uintptr\n\twaiting       *sudog         // sudog structures this g is waiting on (that have a valid elem ptr); in lock order\n\tcgoCtxt       []uintptr      // cgo traceback context\n\tlabels        unsafe.Pointer // profiler labels\n\ttimer         *timer         // cached timer for time.Sleep\n\tsleepWhen     int64          // when to sleep until\n\tselectDone    atomic.Uint32  // are we participating in a select and did someone win the race?\n\n\t// goroutineProfiled indicates the status of this goroutine's stack for the\n\t// current in-progress goroutine profile\n\tgoroutineProfiled goroutineProfileStateHolder\n\n\tcoroarg *coro // argument during coroutine transfers\n\n\t// Per-G tracer state.\n\ttrace gTraceState\n\n\t// Per-G GC state\n\n\t// gcAssistBytes is this G's GC assist credit in terms of\n\t// bytes allocated. If this is positive, then the G has credit\n\t// to allocate gcAssistBytes bytes without assisting. If this\n\t// is negative, then the G must correct this by performing\n\t// scan work. We track this in bytes to make it fast to update\n\t// and check for debt in the malloc hot path. The assist ratio\n\t// determines how this corresponds to scan work debt.\n\tgcAssistBytes int64\n}\n\n// gTrackingPeriod is the number of transitions out of _Grunning between\n// latency tracking runs.\nconst gTrackingPeriod = 8\n\nconst (\n\t// tlsSlots is the number of pointer-sized slots reserved for TLS on some platforms,\n\t// like Windows.\n\ttlsSlots = 6\n\ttlsSize  = tlsSlots * goarch.PtrSize\n)\n\n// Values for m.freeWait.\nconst (\n\tfreeMStack = 0 // M done, free stack and reference.\n\tfreeMRef   = 1 // M done, free reference.\n\tfreeMWait  = 2 // M still in use.\n)\n\ntype m struct {\n\tg0      *g     // goroutine with scheduling stack\n\tmorebuf gobuf  // gobuf arg to morestack\n\tdivmod  uint32 // div/mod denominator for arm - known to liblink\n\t_       uint32 // align next field to 8 bytes\n\n\t// Fields not known to debuggers.\n\tprocid          uint64            // for debuggers, but offset not hard-coded\n\tgsignal         *g                // signal-handling g\n\tgoSigStack      gsignalStack      // Go-allocated signal handling stack\n\tsigmask         sigset            // storage for saved signal mask\n\ttls             [tlsSlots]uintptr // thread-local storage (for x86 extern register)\n\tmstartfn        func()\n\tcurg            *g       // current running goroutine\n\tcaughtsig       guintptr // goroutine running during fatal signal\n\tp               puintptr // attached p for executing go code (nil if not executing go code)\n\tnextp           puintptr\n\toldp            puintptr // the p that was attached before executing a syscall\n\tid              int64\n\tmallocing       int32\n\tthrowing        throwType\n\tpreemptoff      string // if != \"\", keep curg running on this m\n\tlocks           int32\n\tdying           int32\n\tprofilehz       int32\n\tspinning        bool // m is out of work and is actively looking for work\n\tblocked         bool // m is blocked on a note\n\tnewSigstack     bool // minit on C thread called sigaltstack\n\tprintlock       int8\n\tincgo           bool          // m is executing a cgo call\n\tisextra         bool          // m is an extra m\n\tisExtraInC      bool          // m is an extra m that does not have any Go frames\n\tisExtraInSig    bool          // m is an extra m in a signal handler\n\tfreeWait        atomic.Uint32 // Whether it is safe to free g0 and delete m (one of freeMRef, freeMStack, freeMWait)\n\tneedextram      bool\n\tg0StackAccurate bool // whether the g0 stack has accurate bounds\n\ttraceback       uint8\n\tncgocall        uint64        // number of cgo calls in total\n\tncgo            int32         // number of cgo calls currently in progress\n\tcgoCallersUse   atomic.Uint32 // if non-zero, cgoCallers in use temporarily\n\tcgoCallers      *cgoCallers   // cgo traceback if crashing in cgo call\n\tpark            note\n\talllink         *m // on allm\n\tschedlink       muintptr\n\tlockedg         guintptr\n\tcreatestack     [32]uintptr // stack that created this thread, it's used for StackRecord.Stack0, so it must align with it.\n\tlockedExt       uint32      // tracking for external LockOSThread\n\tlockedInt       uint32      // tracking for internal lockOSThread\n\tnextwaitm       muintptr    // next m waiting for lock\n\n\tmLockProfile mLockProfile // fields relating to runtime.lock contention\n\tprofStack    []uintptr    // used for memory/block/mutex stack traces\n\n\t// wait* are used to carry arguments from gopark into park_m, because\n\t// there's no stack to put them on. That is their sole purpose.\n\twaitunlockf          func(*g, unsafe.Pointer) bool\n\twaitlock             unsafe.Pointer\n\twaitTraceSkip        int\n\twaitTraceBlockReason traceBlockReason\n\n\tsyscalltick uint32\n\tfreelink    *m // on sched.freem\n\ttrace       mTraceState\n\n\t// these are here because they are too large to be on the stack\n\t// of low-level NOSPLIT functions.\n\tlibcall    libcall\n\tlibcallpc  uintptr // for cpu profiler\n\tlibcallsp  uintptr\n\tlibcallg   guintptr\n\twinsyscall winlibcall // stores syscall parameters on windows\n\n\tvdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call)\n\tvdsoPC uintptr // PC for traceback while in VDSO call\n\n\t// preemptGen counts the number of completed preemption\n\t// signals. This is used to detect when a preemption is\n\t// requested, but fails.\n\tpreemptGen atomic.Uint32\n\n\t// Whether this is a pending preemption signal on this M.\n\tsignalPending atomic.Uint32\n\n\t// pcvalue lookup cache\n\tpcvalueCache pcvalueCache\n\n\tdlogPerM\n\n\tmOS\n\n\tchacha8   chacha8rand.State\n\tcheaprand uint64\n\n\t// Up to 10 locks held by this m, maintained by the lock ranking code.\n\tlocksHeldLen int\n\tlocksHeld    [10]heldLockInfo\n}\n\ntype p struct {\n\tid          int32\n\tstatus      uint32 // one of pidle/prunning/...\n\tlink        puintptr\n\tschedtick   uint32     // incremented on every scheduler call\n\tsyscalltick uint32     // incremented on every system call\n\tsysmontick  sysmontick // last tick observed by sysmon\n\tm           muintptr   // back-link to associated m (nil if idle)\n\tmcache      *mcache\n\tpcache      pageCache\n\traceprocctx uintptr\n\n\tdeferpool    []*_defer // pool of available defer structs (see panic.go)\n\tdeferpoolbuf [32]*_defer\n\n\t// Cache of goroutine ids, amortizes accesses to runtime\xb7sched.goidgen.\n\tgoidcache    uint64\n\tgoidcacheend uint64\n\n\t// Queue of runnable goroutines. Accessed without lock.\n\trunqhead uint32\n\trunqtail uint32\n\trunq     [256]guintptr\n\t// runnext, if non-nil, is a runnable G that was ready'd by\n\t// the current G and should be run next instead of what's in\n\t// runq if there's time remaining in the running G's time\n\t// slice. It will inherit the time left in the current time\n\t// slice. If a set of goroutines is locked in a\n\t// communicate-and-wait pattern, this schedules that set as a\n\t// unit and eliminates the (potentially large) scheduling\n\t// latency that otherwise arises from adding the ready'd\n\t// goroutines to the end of the run queue.\n\t//\n\t// Note that while other P's may atomically CAS this to zero,\n\t// only the owner P can CAS it to a valid G.\n\trunnext guintptr\n\n\t// Available G's (status == Gdead)\n\tgFree struct {\n\t\tgList\n\t\tn int32\n\t}\n\n\tsudogcache []*sudog\n\tsudogbuf   [128]*sudog\n\n\t// Cache of mspan objects from the heap.\n\tmspancache struct {\n\t\t// We need an explicit length here because this field is used\n\t\t// in allocation codepaths where write barriers are not allowed,\n\t\t// and eliminating the write barrier/keeping it eliminated from\n\t\t// slice updates is tricky, more so than just managing the length\n\t\t// ourselves.\n\t\tlen int\n\t\tbuf [128]*mspan\n\t}\n\n\t// Cache of a single pinner object to reduce allocations from repeated\n\t// pinner creation.\n\tpinnerCache *pinner\n\n\ttrace pTraceState\n\n\tpalloc persistentAlloc // per-P to avoid mutex\n\n\t// Per-P GC state\n\tgcAssistTime         int64 // Nanoseconds in assistAlloc\n\tgcFractionalMarkTime int64 // Nanoseconds in fractional mark worker (atomic)\n\n\t// limiterEvent tracks events for the GC CPU limiter.\n\tlimiterEvent limiterEvent\n\n\t// gcMarkWorkerMode is the mode for the next mark worker to run in.\n\t// That is, this is used to communicate with the worker goroutine\n\t// selected for immediate execution by\n\t// gcController.findRunnableGCWorker. When scheduling other goroutines,\n\t// this field must be set to gcMarkWorkerNotWorker.\n\tgcMarkWorkerMode gcMarkWorkerMode\n\t// gcMarkWorkerStartTime is the nanotime() at which the most recent\n\t// mark worker started.\n\tgcMarkWorkerStartTime int64\n\n\t// gcw is this P's GC work buffer cache. The work buffer is\n\t// filled by write barriers, drained by mutator assists, and\n\t// disposed on certain GC state transitions.\n\tgcw gcWork\n\n\t// wbBuf is this P's GC write barrier buffer.\n\t//\n\t// TODO: Consider caching this in the running G.\n\twbBuf wbBuf\n\n\trunSafePointFn uint32 // if 1, run sched.safePointFn at next safe point\n\n\t// statsSeq is a counter indicating whether this P is currently\n\t// writing any stats. Its value is even when not, odd when it is.\n\tstatsSeq atomic.Uint32\n\n\t// Timer heap.\n\ttimers timers\n\n\t// maxStackScanDelta accumulates the amount of stack space held by\n\t// live goroutines (i.e. those eligible for stack scanning).\n\t// Flushed to gcController.maxStackScan once maxStackScanSlack\n\t// or -maxStackScanSlack is reached.\n\tmaxStackScanDelta int64\n\n\t// gc-time statistics about current goroutines\n\t// Note that this differs from maxStackScan in that this\n\t// accumulates the actual stack observed to be used at GC time (hi - sp),\n\t// not an instantaneous measure of the total stack size that might need\n\t// to be scanned (hi - lo).\n\tscannedStackSize uint64 // stack size of goroutines scanned by this P\n\tscannedStacks    uint64 // number of goroutines scanned by this P\n\n\t// preempt is set to indicate that this P should be enter the\n\t// scheduler ASAP (regardless of what G is running on it).\n\tpreempt bool\n\n\t// gcStopTime is the nanotime timestamp that this P last entered _Pgcstop.\n\tgcStopTime int64\n\n\t// Padding is no longer needed. False sharing is now not a worry because p is large enough\n\t// that its size class is an integer multiple of the cache line size (for any of our architectures).\n}\n\ntype schedt struct {\n\tgoidgen   atomic.Uint64\n\tlastpoll  atomic.Int64 // time of last network poll, 0 if currently polling\n\tpollUntil atomic.Int64 // time to which current poll is sleeping\n\n\tlock mutex\n\n\t// When increasing nmidle, nmidlelocked, nmsys, or nmfreed, be\n\t// sure to call checkdead().\n\n\tmidle        muintptr // idle m's waiting for work\n\tnmidle       int32    // number of idle m's waiting for work\n\tnmidlelocked int32    // number of locked m's waiting for work\n\tmnext        int64    // number of m's that have been created and next M ID\n\tmaxmcount    int32    // maximum number of m's allowed (or die)\n\tnmsys        int32    // number of system m's not counted for deadlock\n\tnmfreed      int64    // cumulative number of freed m's\n\n\tngsys atomic.Int32 // number of system goroutines\n\n\tpidle        puintptr // idle p's\n\tnpidle       atomic.Int32\n\tnmspinning   atomic.Int32  // See \"Worker thread parking/unparking\" comment in proc.go.\n\tneedspinning atomic.Uint32 // See \"Delicate dance\" comment in proc.go. Boolean. Must hold sched.lock to set to 1.\n\n\t// Global runnable queue.\n\trunq     gQueue\n\trunqsize int32\n\n\t// disable controls selective disabling of the scheduler.\n\t//\n\t// Use schedEnableUser to control this.\n\t//\n\t// disable is protected by sched.lock.\n\tdisable struct {\n\t\t// user disables scheduling of user goroutines.\n\t\tuser     bool\n\t\trunnable gQueue // pending runnable Gs\n\t\tn        int32  // length of runnable\n\t}\n\n\t// Global cache of dead G's.\n\tgFree struct {\n\t\tlock    mutex\n\t\tstack   gList // Gs with stacks\n\t\tnoStack gList // Gs without stacks\n\t\tn       int32\n\t}\n\n\t// Central cache of sudog structs.\n\tsudoglock  mutex\n\tsudogcache *sudog\n\n\t// Central pool of available defer structs.\n\tdeferlock mutex\n\tdeferpool *_defer\n\n\t// freem is the list of m's waiting to be freed when their\n\t// m.exited is set. Linked through m.freelink.\n\tfreem *m\n\n\tgcwaiting  atomic.Bool // gc is waiting to run\n\tstopwait   int32\n\tstopnote   note\n\tsysmonwait atomic.Bool\n\tsysmonnote note\n\n\t// safePointFn should be called on each P at the next GC\n\t// safepoint if p.runSafePointFn is set.\n\tsafePointFn   func(*p)\n\tsafePointWait int32\n\tsafePointNote note\n\n\tprofilehz int32 // cpu profiling rate\n\n\tprocresizetime int64 // nanotime() of last change to gomaxprocs\n\ttotaltime      int64 // \u222bgomaxprocs dt up to procresizetime\n\n\t// sysmonlock protects sysmon's actions on the runtime.\n\t//\n\t// Acquire and hold this mutex to block sysmon from interacting\n\t// with the rest of the runtime.\n\tsysmonlock mutex\n\n\t// timeToRun is a distribution of scheduling latencies, defined\n\t// as the sum of time a G spends in the _Grunnable state before\n\t// it transitions to _Grunning.\n\ttimeToRun timeHistogram\n\n\t// idleTime is the total CPU time Ps have \"spent\" idle.\n\t//\n\t// Reset on each GC cycle.\n\tidleTime atomic.Int64\n\n\t// totalMutexWaitTime is the sum of time goroutines have spent in _Gwaiting\n\t// with a waitreason of the form waitReasonSync{RW,}Mutex{R,}Lock.\n\ttotalMutexWaitTime atomic.Int64\n\n\t// stwStoppingTimeGC/Other are distributions of stop-the-world stopping\n\t// latencies, defined as the time taken by stopTheWorldWithSema to get\n\t// all Ps to stop. stwStoppingTimeGC covers all GC-related STWs,\n\t// stwStoppingTimeOther covers the others.\n\tstwStoppingTimeGC    timeHistogram\n\tstwStoppingTimeOther timeHistogram\n\n\t// stwTotalTimeGC/Other are distributions of stop-the-world total\n\t// latencies, defined as the total time from stopTheWorldWithSema to\n\t// startTheWorldWithSema. This is a superset of\n\t// stwStoppingTimeGC/Other. stwTotalTimeGC covers all GC-related STWs,\n\t// stwTotalTimeOther covers the others.\n\tstwTotalTimeGC    timeHistogram\n\tstwTotalTimeOther timeHistogram\n\n\t// totalRuntimeLockWaitTime (plus the value of lockWaitTime on each M in\n\t// allm) is the sum of time goroutines have spent in _Grunnable and with an\n\t// M, but waiting for locks within the runtime. This field stores the value\n\t// for Ms that have exited.\n\ttotalRuntimeLockWaitTime atomic.Int64\n}\n"})})]})}function d(t={}){const{wrapper:n}={...(0,a.R)(),...t.components};return n?(0,o.jsx)(n,{...t,children:(0,o.jsx)(u,{...t})}):u(t)}},8453:(t,n,e)=>{e.d(n,{R:()=>s,x:()=>r});var i=e(6540);const o={},a=i.createContext(o);function s(t){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof t?t(n):{...n,...t}}),[n,t])}function r(t){let n;return n=t.disableParentContext?"function"==typeof t.components?t.components(o):t.components||o:s(t.components),i.createElement(a.Provider,{value:n},t.children)}}}]);